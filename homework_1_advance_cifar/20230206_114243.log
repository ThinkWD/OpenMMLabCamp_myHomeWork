2023-02-06 11:42:43,804 - mmcls - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.13 (default, Mar 28 2022, 11:38:47) [GCC 7.5.0]
CUDA available: True
GPU 0: NVIDIA GeForce RTX 3080
CUDA_HOME: /usr/local/cuda-11.3
NVCC: Cuda compilation tools, release 11.3, V11.3.109
GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
PyTorch: 1.12.1
PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.13.1
OpenCV: 4.6.0
MMCV: 1.6.1
MMCV Compiler: GCC 9.3
MMCV CUDA Compiler: 11.3
MMClassification: 0.25.0+
------------------------------------------------------------

2023-02-06 11:42:43,804 - mmcls - INFO - Distributed training: False
2023-02-06 11:42:43,909 - mmcls - INFO - Config:
task_type = 'homework_1_advance_cifar'
work_dir = 'work_dirs/homework_1_advance_cifar'
imgs_prefix = 'data/homework_1_advance_cifar/cifar10'
num_classes = 10
gpu_batch_size = 64
load_from = 'checkpoints/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth'
resume_from = None
model = dict(
    type='ImageClassifier',
    backbone=dict(
        type='SwinTransformer', arch='tiny', img_size=224, drop_path_rate=0.2),
    neck=dict(type='GlobalAveragePooling'),
    head=dict(
        type='LinearClsHead',
        num_classes=10,
        in_channels=768,
        init_cfg=None,
        loss=dict(
            type='LabelSmoothLoss', label_smooth_val=0.1, mode='original'),
        cal_acc=False),
    init_cfg=[
        dict(type='TruncNormal', layer='Linear', std=0.02, bias=0.0),
        dict(type='Constant', layer='LayerNorm', val=1.0, bias=0.0)
    ],
    train_cfg=dict(augments=[
        dict(type='BatchMixup', alpha=0.8, num_classes=10, prob=0.5),
        dict(type='BatchCutMix', alpha=1.0, num_classes=10, prob=0.5)
    ]))
dataset_type = 'CIFAR10'
img_norm_cfg = dict(
    mean=[125.307, 122.961, 113.8575],
    std=[51.5865, 50.847, 51.255],
    to_rgb=False)
train_pipeline = [
    dict(type='RandomCrop', size=32, padding=4),
    dict(type='RandomFlip', flip_prob=0.5, direction='horizontal'),
    dict(type='Resize', size=224),
    dict(
        type='Normalize',
        mean=[125.307, 122.961, 113.8575],
        std=[51.5865, 50.847, 51.255],
        to_rgb=False),
    dict(type='ImageToTensor', keys=['img']),
    dict(type='ToTensor', keys=['gt_label']),
    dict(type='Collect', keys=['img', 'gt_label'])
]
test_pipeline = [
    dict(type='Resize', size=224),
    dict(
        type='Normalize',
        mean=[125.307, 122.961, 113.8575],
        std=[51.5865, 50.847, 51.255],
        to_rgb=False),
    dict(type='ImageToTensor', keys=['img']),
    dict(type='Collect', keys=['img'])
]
data = dict(
    samples_per_gpu=64,
    workers_per_gpu=2,
    train=dict(
        type='CIFAR10',
        data_prefix='data/homework_1_advance_cifar/cifar10',
        pipeline=[
            dict(type='RandomCrop', size=32, padding=4),
            dict(type='RandomFlip', flip_prob=0.5, direction='horizontal'),
            dict(type='Resize', size=224),
            dict(
                type='Normalize',
                mean=[125.307, 122.961, 113.8575],
                std=[51.5865, 50.847, 51.255],
                to_rgb=False),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='ToTensor', keys=['gt_label']),
            dict(type='Collect', keys=['img', 'gt_label'])
        ]),
    val=dict(
        type='CIFAR10',
        data_prefix='data/homework_1_advance_cifar/cifar10',
        pipeline=[
            dict(type='Resize', size=224),
            dict(
                type='Normalize',
                mean=[125.307, 122.961, 113.8575],
                std=[51.5865, 50.847, 51.255],
                to_rgb=False),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ],
        test_mode=True),
    test=dict(
        type='CIFAR10',
        data_prefix='data/homework_1_advance_cifar/cifar10',
        pipeline=[
            dict(type='Resize', size=224),
            dict(
                type='Normalize',
                mean=[125.307, 122.961, 113.8575],
                std=[51.5865, 50.847, 51.255],
                to_rgb=False),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ],
        test_mode=True))
optimizer = dict(type='SGD', lr=0.001, momentum=0.9, weight_decay=0.0001)
optimizer_config = dict(grad_clip=None)
lr_config = dict(policy='step', step=[3, 6, 9])
runner = dict(type='EpochBasedRunner', max_epochs=10)
evaluation = dict(interval=1, metric='accuracy')
checkpoint_config = dict(interval=1)
log_config = dict(interval=20, hooks=[dict(type='TextLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
workflow = [('train', 1)]
gpu_ids = [0]

2023-02-06 11:42:43,910 - mmcls - INFO - Set random seed to 628134672, deterministic: False
2023-02-06 11:42:44,029 - mmcls - INFO - initialize ImageClassifier with init_cfg [{'type': 'TruncNormal', 'layer': 'Linear', 'std': 0.02, 'bias': 0.0}, {'type': 'Constant', 'layer': 'LayerNorm', 'val': 1.0, 'bias': 0.0}]
Name of parameter - Initialization information

backbone.patch_embed.projection.weight - torch.Size([96, 3, 4, 4]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.patch_embed.projection.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.patch_embed.norm.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.patch_embed.norm.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.0.blocks.0.norm1.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.0.blocks.0.norm1.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 3]): 
Initialized by user-defined `init_weights` in WindowMSA  

backbone.stages.0.blocks.0.attn.w_msa.qkv.weight - torch.Size([288, 96]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.0.blocks.0.attn.w_msa.qkv.bias - torch.Size([288]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.0.blocks.0.attn.w_msa.proj.weight - torch.Size([96, 96]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.0.blocks.0.attn.w_msa.proj.bias - torch.Size([96]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.0.blocks.0.norm2.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.0.blocks.0.norm2.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.0.blocks.0.ffn.layers.0.0.weight - torch.Size([384, 96]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.0.blocks.0.ffn.layers.0.0.bias - torch.Size([384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.0.blocks.0.ffn.layers.1.weight - torch.Size([96, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.0.blocks.0.ffn.layers.1.bias - torch.Size([96]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.0.blocks.1.norm1.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.0.blocks.1.norm1.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 3]): 
Initialized by user-defined `init_weights` in WindowMSA  

backbone.stages.0.blocks.1.attn.w_msa.qkv.weight - torch.Size([288, 96]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.0.blocks.1.attn.w_msa.qkv.bias - torch.Size([288]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.0.blocks.1.attn.w_msa.proj.weight - torch.Size([96, 96]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.0.blocks.1.attn.w_msa.proj.bias - torch.Size([96]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.0.blocks.1.norm2.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.0.blocks.1.norm2.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.0.blocks.1.ffn.layers.0.0.weight - torch.Size([384, 96]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.0.blocks.1.ffn.layers.0.0.bias - torch.Size([384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.0.blocks.1.ffn.layers.1.weight - torch.Size([96, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.0.blocks.1.ffn.layers.1.bias - torch.Size([96]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.0.downsample.norm.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.0.downsample.norm.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.0.downsample.reduction.weight - torch.Size([192, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.1.blocks.0.norm1.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.1.blocks.0.norm1.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 6]): 
Initialized by user-defined `init_weights` in WindowMSA  

backbone.stages.1.blocks.0.attn.w_msa.qkv.weight - torch.Size([576, 192]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.1.blocks.0.attn.w_msa.qkv.bias - torch.Size([576]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.1.blocks.0.attn.w_msa.proj.weight - torch.Size([192, 192]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.1.blocks.0.attn.w_msa.proj.bias - torch.Size([192]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.1.blocks.0.norm2.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.1.blocks.0.norm2.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.1.blocks.0.ffn.layers.0.0.weight - torch.Size([768, 192]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.1.blocks.0.ffn.layers.0.0.bias - torch.Size([768]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.1.blocks.0.ffn.layers.1.weight - torch.Size([192, 768]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.1.blocks.0.ffn.layers.1.bias - torch.Size([192]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.1.blocks.1.norm1.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.1.blocks.1.norm1.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 6]): 
Initialized by user-defined `init_weights` in WindowMSA  

backbone.stages.1.blocks.1.attn.w_msa.qkv.weight - torch.Size([576, 192]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.1.blocks.1.attn.w_msa.qkv.bias - torch.Size([576]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.1.blocks.1.attn.w_msa.proj.weight - torch.Size([192, 192]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.1.blocks.1.attn.w_msa.proj.bias - torch.Size([192]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.1.blocks.1.norm2.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.1.blocks.1.norm2.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.1.blocks.1.ffn.layers.0.0.weight - torch.Size([768, 192]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.1.blocks.1.ffn.layers.0.0.bias - torch.Size([768]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.1.blocks.1.ffn.layers.1.weight - torch.Size([192, 768]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.1.blocks.1.ffn.layers.1.bias - torch.Size([192]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.1.downsample.norm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.1.downsample.norm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.1.downsample.reduction.weight - torch.Size([384, 768]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.0.norm1.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.0.norm1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in WindowMSA  

backbone.stages.2.blocks.0.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.0.attn.w_msa.qkv.bias - torch.Size([1152]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.0.attn.w_msa.proj.weight - torch.Size([384, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.0.attn.w_msa.proj.bias - torch.Size([384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.0.norm2.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.0.norm2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.0.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.0.ffn.layers.0.0.bias - torch.Size([1536]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.0.ffn.layers.1.weight - torch.Size([384, 1536]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.0.ffn.layers.1.bias - torch.Size([384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.1.norm1.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.1.norm1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in WindowMSA  

backbone.stages.2.blocks.1.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.1.attn.w_msa.qkv.bias - torch.Size([1152]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.1.attn.w_msa.proj.weight - torch.Size([384, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.1.attn.w_msa.proj.bias - torch.Size([384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.1.norm2.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.1.norm2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.1.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.1.ffn.layers.0.0.bias - torch.Size([1536]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.1.ffn.layers.1.weight - torch.Size([384, 1536]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.1.ffn.layers.1.bias - torch.Size([384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.2.norm1.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.2.norm1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in WindowMSA  

backbone.stages.2.blocks.2.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.2.attn.w_msa.qkv.bias - torch.Size([1152]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.2.attn.w_msa.proj.weight - torch.Size([384, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.2.attn.w_msa.proj.bias - torch.Size([384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.2.norm2.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.2.norm2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.2.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.2.ffn.layers.0.0.bias - torch.Size([1536]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.2.ffn.layers.1.weight - torch.Size([384, 1536]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.2.ffn.layers.1.bias - torch.Size([384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.3.norm1.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.3.norm1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in WindowMSA  

backbone.stages.2.blocks.3.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.3.attn.w_msa.qkv.bias - torch.Size([1152]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.3.attn.w_msa.proj.weight - torch.Size([384, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.3.attn.w_msa.proj.bias - torch.Size([384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.3.norm2.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.3.norm2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.3.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.3.ffn.layers.0.0.bias - torch.Size([1536]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.3.ffn.layers.1.weight - torch.Size([384, 1536]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.3.ffn.layers.1.bias - torch.Size([384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.4.norm1.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.4.norm1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in WindowMSA  

backbone.stages.2.blocks.4.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.4.attn.w_msa.qkv.bias - torch.Size([1152]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.4.attn.w_msa.proj.weight - torch.Size([384, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.4.attn.w_msa.proj.bias - torch.Size([384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.4.norm2.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.4.norm2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.4.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.4.ffn.layers.0.0.bias - torch.Size([1536]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.4.ffn.layers.1.weight - torch.Size([384, 1536]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.4.ffn.layers.1.bias - torch.Size([384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.5.norm1.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.5.norm1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in WindowMSA  

backbone.stages.2.blocks.5.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.5.attn.w_msa.qkv.bias - torch.Size([1152]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.5.attn.w_msa.proj.weight - torch.Size([384, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.5.attn.w_msa.proj.bias - torch.Size([384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.5.norm2.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.5.norm2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.5.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.5.ffn.layers.0.0.bias - torch.Size([1536]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.5.ffn.layers.1.weight - torch.Size([384, 1536]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.5.ffn.layers.1.bias - torch.Size([384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.downsample.norm.weight - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.downsample.norm.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.downsample.reduction.weight - torch.Size([768, 1536]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.3.blocks.0.norm1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.3.blocks.0.norm1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in WindowMSA  

backbone.stages.3.blocks.0.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.3.blocks.0.attn.w_msa.qkv.bias - torch.Size([2304]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.3.blocks.0.attn.w_msa.proj.weight - torch.Size([768, 768]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.3.blocks.0.attn.w_msa.proj.bias - torch.Size([768]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.3.blocks.0.norm2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.3.blocks.0.norm2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.3.blocks.0.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.3.blocks.0.ffn.layers.0.0.bias - torch.Size([3072]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.3.blocks.0.ffn.layers.1.weight - torch.Size([768, 3072]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.3.blocks.0.ffn.layers.1.bias - torch.Size([768]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.3.blocks.1.norm1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.3.blocks.1.norm1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in WindowMSA  

backbone.stages.3.blocks.1.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.3.blocks.1.attn.w_msa.qkv.bias - torch.Size([2304]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.3.blocks.1.attn.w_msa.proj.weight - torch.Size([768, 768]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.3.blocks.1.attn.w_msa.proj.bias - torch.Size([768]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.3.blocks.1.norm2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.3.blocks.1.norm2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.3.blocks.1.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.3.blocks.1.ffn.layers.0.0.bias - torch.Size([3072]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.3.blocks.1.ffn.layers.1.weight - torch.Size([768, 3072]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.3.blocks.1.ffn.layers.1.bias - torch.Size([768]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.norm3.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.norm3.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

head.fc.weight - torch.Size([10, 768]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

head.fc.bias - torch.Size([10]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 
2023-02-06 11:42:45,375 - mmcls - INFO - load checkpoint from local path: checkpoints/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth
2023-02-06 11:42:45,418 - mmcls - WARNING - The model and loaded state dict do not match exactly

size mismatch for head.fc.weight: copying a param with shape torch.Size([1000, 768]) from checkpoint, the shape in current model is torch.Size([10, 768]).
size mismatch for head.fc.bias: copying a param with shape torch.Size([1000]) from checkpoint, the shape in current model is torch.Size([10]).
2023-02-06 11:42:45,418 - mmcls - INFO - Start running, host: newu@newu-black, work_dir: /home/lx_dir/mmcls/work_dirs/homework_1_advance_cifar
2023-02-06 11:42:45,418 - mmcls - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) StepLrUpdaterHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) StepLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) StepLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2023-02-06 11:42:45,418 - mmcls - INFO - workflow: [('train', 1)], max: 10 epochs
2023-02-06 11:42:45,418 - mmcls - INFO - Checkpoints will be saved to /home/lx_dir/mmcls/work_dirs/homework_1_advance_cifar by HardDiskBackend.
2023-02-06 11:42:53,991 - mmcls - INFO - Epoch [1][20/782]	lr: 1.000e-03, eta: 0:55:13, time: 0.425, data_time: 0.108, memory: 6926, loss: 2.2952
2023-02-06 11:42:59,164 - mmcls - INFO - Epoch [1][40/782]	lr: 1.000e-03, eta: 0:44:18, time: 0.259, data_time: 0.004, memory: 6926, loss: 2.0947
2023-02-06 11:43:04,344 - mmcls - INFO - Epoch [1][60/782]	lr: 1.000e-03, eta: 0:40:37, time: 0.259, data_time: 0.004, memory: 6926, loss: 1.8965
2023-02-06 11:43:09,535 - mmcls - INFO - Epoch [1][80/782]	lr: 1.000e-03, eta: 0:38:45, time: 0.260, data_time: 0.004, memory: 6926, loss: 1.6503
2023-02-06 11:43:14,740 - mmcls - INFO - Epoch [1][100/782]	lr: 1.000e-03, eta: 0:37:37, time: 0.260, data_time: 0.004, memory: 6926, loss: 1.5897
2023-02-06 11:43:19,950 - mmcls - INFO - Epoch [1][120/782]	lr: 1.000e-03, eta: 0:36:50, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.5762
2023-02-06 11:43:25,165 - mmcls - INFO - Epoch [1][140/782]	lr: 1.000e-03, eta: 0:36:16, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.4778
2023-02-06 11:43:30,380 - mmcls - INFO - Epoch [1][160/782]	lr: 1.000e-03, eta: 0:35:48, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.5027
2023-02-06 11:43:35,598 - mmcls - INFO - Epoch [1][180/782]	lr: 1.000e-03, eta: 0:35:26, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.4393
2023-02-06 11:43:40,821 - mmcls - INFO - Epoch [1][200/782]	lr: 1.000e-03, eta: 0:35:07, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.3574
2023-02-06 11:43:46,046 - mmcls - INFO - Epoch [1][220/782]	lr: 1.000e-03, eta: 0:34:51, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.3858
2023-02-06 11:43:51,270 - mmcls - INFO - Epoch [1][240/782]	lr: 1.000e-03, eta: 0:34:37, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.3994
2023-02-06 11:43:56,497 - mmcls - INFO - Epoch [1][260/782]	lr: 1.000e-03, eta: 0:34:24, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.3824
2023-02-06 11:44:01,726 - mmcls - INFO - Epoch [1][280/782]	lr: 1.000e-03, eta: 0:34:12, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.3868
2023-02-06 11:44:06,957 - mmcls - INFO - Epoch [1][300/782]	lr: 1.000e-03, eta: 0:34:01, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.3677
2023-02-06 11:44:12,191 - mmcls - INFO - Epoch [1][320/782]	lr: 1.000e-03, eta: 0:33:51, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.4220
2023-02-06 11:44:17,422 - mmcls - INFO - Epoch [1][340/782]	lr: 1.000e-03, eta: 0:33:42, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.4177
2023-02-06 11:44:22,655 - mmcls - INFO - Epoch [1][360/782]	lr: 1.000e-03, eta: 0:33:33, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.3218
2023-02-06 11:44:27,889 - mmcls - INFO - Epoch [1][380/782]	lr: 1.000e-03, eta: 0:33:24, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.4393
2023-02-06 11:44:33,124 - mmcls - INFO - Epoch [1][400/782]	lr: 1.000e-03, eta: 0:33:16, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2828
2023-02-06 11:44:38,359 - mmcls - INFO - Epoch [1][420/782]	lr: 1.000e-03, eta: 0:33:08, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.3157
2023-02-06 11:44:43,595 - mmcls - INFO - Epoch [1][440/782]	lr: 1.000e-03, eta: 0:33:00, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.3120
2023-02-06 11:44:48,831 - mmcls - INFO - Epoch [1][460/782]	lr: 1.000e-03, eta: 0:32:53, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.3024
2023-02-06 11:44:54,071 - mmcls - INFO - Epoch [1][480/782]	lr: 1.000e-03, eta: 0:32:46, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.3913
2023-02-06 11:44:59,306 - mmcls - INFO - Epoch [1][500/782]	lr: 1.000e-03, eta: 0:32:38, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2657
2023-02-06 11:45:04,541 - mmcls - INFO - Epoch [1][520/782]	lr: 1.000e-03, eta: 0:32:31, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.3371
2023-02-06 11:45:09,778 - mmcls - INFO - Epoch [1][540/782]	lr: 1.000e-03, eta: 0:32:25, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1495
2023-02-06 11:45:15,015 - mmcls - INFO - Epoch [1][560/782]	lr: 1.000e-03, eta: 0:32:18, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.3178
2023-02-06 11:45:20,252 - mmcls - INFO - Epoch [1][580/782]	lr: 1.000e-03, eta: 0:32:11, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.3339
2023-02-06 11:45:25,488 - mmcls - INFO - Epoch [1][600/782]	lr: 1.000e-03, eta: 0:32:05, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2657
2023-02-06 11:45:30,726 - mmcls - INFO - Epoch [1][620/782]	lr: 1.000e-03, eta: 0:31:58, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.3304
2023-02-06 11:45:35,964 - mmcls - INFO - Epoch [1][640/782]	lr: 1.000e-03, eta: 0:31:52, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2625
2023-02-06 11:45:41,202 - mmcls - INFO - Epoch [1][660/782]	lr: 1.000e-03, eta: 0:31:46, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2637
2023-02-06 11:45:46,439 - mmcls - INFO - Epoch [1][680/782]	lr: 1.000e-03, eta: 0:31:39, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.3849
2023-02-06 11:45:51,679 - mmcls - INFO - Epoch [1][700/782]	lr: 1.000e-03, eta: 0:31:33, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1522
2023-02-06 11:45:56,916 - mmcls - INFO - Epoch [1][720/782]	lr: 1.000e-03, eta: 0:31:27, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.3702
2023-02-06 11:46:02,155 - mmcls - INFO - Epoch [1][740/782]	lr: 1.000e-03, eta: 0:31:21, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.3443
2023-02-06 11:46:07,393 - mmcls - INFO - Epoch [1][760/782]	lr: 1.000e-03, eta: 0:31:15, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2049
2023-02-06 11:46:12,632 - mmcls - INFO - Epoch [1][780/782]	lr: 1.000e-03, eta: 0:31:09, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2604
2023-02-06 11:46:12,945 - mmcls - INFO - Saving checkpoint at 1 epochs
2023-02-06 11:46:26,872 - mmcls - INFO - Epoch(val) [1][157]	accuracy_top-1: 94.3400, accuracy_top-5: 99.8900
2023-02-06 11:46:34,156 - mmcls - INFO - Epoch [2][20/782]	lr: 1.000e-03, eta: 0:31:15, time: 0.360, data_time: 0.103, memory: 6926, loss: 1.2047
2023-02-06 11:46:39,389 - mmcls - INFO - Epoch [2][40/782]	lr: 1.000e-03, eta: 0:31:09, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2612
2023-02-06 11:46:44,623 - mmcls - INFO - Epoch [2][60/782]	lr: 1.000e-03, eta: 0:31:03, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.3053
2023-02-06 11:46:49,857 - mmcls - INFO - Epoch [2][80/782]	lr: 1.000e-03, eta: 0:30:56, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2763
2023-02-06 11:46:55,089 - mmcls - INFO - Epoch [2][100/782]	lr: 1.000e-03, eta: 0:30:50, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1057
2023-02-06 11:47:00,327 - mmcls - INFO - Epoch [2][120/782]	lr: 1.000e-03, eta: 0:30:44, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.3726
2023-02-06 11:47:05,563 - mmcls - INFO - Epoch [2][140/782]	lr: 1.000e-03, eta: 0:30:38, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2303
2023-02-06 11:47:10,800 - mmcls - INFO - Epoch [2][160/782]	lr: 1.000e-03, eta: 0:30:32, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1997
2023-02-06 11:47:16,036 - mmcls - INFO - Epoch [2][180/782]	lr: 1.000e-03, eta: 0:30:26, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2975
2023-02-06 11:47:21,271 - mmcls - INFO - Epoch [2][200/782]	lr: 1.000e-03, eta: 0:30:20, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2751
2023-02-06 11:47:26,508 - mmcls - INFO - Epoch [2][220/782]	lr: 1.000e-03, eta: 0:30:14, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.3488
2023-02-06 11:47:31,749 - mmcls - INFO - Epoch [2][240/782]	lr: 1.000e-03, eta: 0:30:08, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2896
2023-02-06 11:47:36,986 - mmcls - INFO - Epoch [2][260/782]	lr: 1.000e-03, eta: 0:30:02, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2628
2023-02-06 11:47:42,224 - mmcls - INFO - Epoch [2][280/782]	lr: 1.000e-03, eta: 0:29:57, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2037
2023-02-06 11:47:47,460 - mmcls - INFO - Epoch [2][300/782]	lr: 1.000e-03, eta: 0:29:51, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2988
2023-02-06 11:47:52,694 - mmcls - INFO - Epoch [2][320/782]	lr: 1.000e-03, eta: 0:29:45, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1694
2023-02-06 11:47:57,932 - mmcls - INFO - Epoch [2][340/782]	lr: 1.000e-03, eta: 0:29:39, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1852
2023-02-06 11:48:03,170 - mmcls - INFO - Epoch [2][360/782]	lr: 1.000e-03, eta: 0:29:33, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2173
2023-02-06 11:48:08,408 - mmcls - INFO - Epoch [2][380/782]	lr: 1.000e-03, eta: 0:29:28, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2865
2023-02-06 11:48:13,644 - mmcls - INFO - Epoch [2][400/782]	lr: 1.000e-03, eta: 0:29:22, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2525
2023-02-06 11:48:18,880 - mmcls - INFO - Epoch [2][420/782]	lr: 1.000e-03, eta: 0:29:16, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2145
2023-02-06 11:48:24,113 - mmcls - INFO - Epoch [2][440/782]	lr: 1.000e-03, eta: 0:29:10, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2739
2023-02-06 11:48:29,350 - mmcls - INFO - Epoch [2][460/782]	lr: 1.000e-03, eta: 0:29:05, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1910
2023-02-06 11:48:34,585 - mmcls - INFO - Epoch [2][480/782]	lr: 1.000e-03, eta: 0:28:59, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2680
2023-02-06 11:48:39,821 - mmcls - INFO - Epoch [2][500/782]	lr: 1.000e-03, eta: 0:28:53, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2264
2023-02-06 11:48:45,054 - mmcls - INFO - Epoch [2][520/782]	lr: 1.000e-03, eta: 0:28:48, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2838
2023-02-06 11:48:50,289 - mmcls - INFO - Epoch [2][540/782]	lr: 1.000e-03, eta: 0:28:42, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.0815
2023-02-06 11:48:55,525 - mmcls - INFO - Epoch [2][560/782]	lr: 1.000e-03, eta: 0:28:37, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1031
2023-02-06 11:49:00,762 - mmcls - INFO - Epoch [2][580/782]	lr: 1.000e-03, eta: 0:28:31, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2609
2023-02-06 11:49:05,996 - mmcls - INFO - Epoch [2][600/782]	lr: 1.000e-03, eta: 0:28:25, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2242
2023-02-06 11:49:11,233 - mmcls - INFO - Epoch [2][620/782]	lr: 1.000e-03, eta: 0:28:20, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1357
2023-02-06 11:49:16,468 - mmcls - INFO - Epoch [2][640/782]	lr: 1.000e-03, eta: 0:28:14, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1746
2023-02-06 11:49:21,706 - mmcls - INFO - Epoch [2][660/782]	lr: 1.000e-03, eta: 0:28:09, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2909
2023-02-06 11:49:26,943 - mmcls - INFO - Epoch [2][680/782]	lr: 1.000e-03, eta: 0:28:03, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2047
2023-02-06 11:49:32,177 - mmcls - INFO - Epoch [2][700/782]	lr: 1.000e-03, eta: 0:27:57, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2747
2023-02-06 11:49:37,410 - mmcls - INFO - Epoch [2][720/782]	lr: 1.000e-03, eta: 0:27:52, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2177
2023-02-06 11:49:42,642 - mmcls - INFO - Epoch [2][740/782]	lr: 1.000e-03, eta: 0:27:46, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1732
2023-02-06 11:49:47,876 - mmcls - INFO - Epoch [2][760/782]	lr: 1.000e-03, eta: 0:27:41, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1482
2023-02-06 11:49:53,110 - mmcls - INFO - Epoch [2][780/782]	lr: 1.000e-03, eta: 0:27:35, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2397
2023-02-06 11:49:53,421 - mmcls - INFO - Saving checkpoint at 2 epochs
2023-02-06 11:59:06,272 - mmcls - INFO - Epoch(val) [2][157]	accuracy_top-1: 95.7100, accuracy_top-5: 99.9600
2023-02-06 11:59:13,449 - mmcls - INFO - Epoch [3][20/782]	lr: 1.000e-03, eta: 0:27:35, time: 0.355, data_time: 0.103, memory: 6926, loss: 1.2675
2023-02-06 11:59:18,581 - mmcls - INFO - Epoch [3][40/782]	lr: 1.000e-03, eta: 0:27:29, time: 0.257, data_time: 0.004, memory: 6926, loss: 1.1748
2023-02-06 11:59:23,729 - mmcls - INFO - Epoch [3][60/782]	lr: 1.000e-03, eta: 0:27:23, time: 0.257, data_time: 0.004, memory: 6926, loss: 1.1402
2023-02-06 11:59:28,889 - mmcls - INFO - Epoch [3][80/782]	lr: 1.000e-03, eta: 0:27:17, time: 0.258, data_time: 0.004, memory: 6926, loss: 1.2604
2023-02-06 11:59:34,056 - mmcls - INFO - Epoch [3][100/782]	lr: 1.000e-03, eta: 0:27:11, time: 0.258, data_time: 0.004, memory: 6926, loss: 1.2743
2023-02-06 11:59:39,223 - mmcls - INFO - Epoch [3][120/782]	lr: 1.000e-03, eta: 0:27:05, time: 0.258, data_time: 0.004, memory: 6926, loss: 1.1789
2023-02-06 11:59:44,391 - mmcls - INFO - Epoch [3][140/782]	lr: 1.000e-03, eta: 0:26:59, time: 0.258, data_time: 0.004, memory: 6926, loss: 1.1884
2023-02-06 11:59:49,568 - mmcls - INFO - Epoch [3][160/782]	lr: 1.000e-03, eta: 0:26:54, time: 0.259, data_time: 0.004, memory: 6926, loss: 1.2279
2023-02-06 11:59:54,749 - mmcls - INFO - Epoch [3][180/782]	lr: 1.000e-03, eta: 0:26:48, time: 0.259, data_time: 0.004, memory: 6926, loss: 1.2068
2023-02-06 11:59:59,940 - mmcls - INFO - Epoch [3][200/782]	lr: 1.000e-03, eta: 0:26:42, time: 0.260, data_time: 0.004, memory: 6926, loss: 1.2900
2023-02-06 12:00:05,131 - mmcls - INFO - Epoch [3][220/782]	lr: 1.000e-03, eta: 0:26:37, time: 0.260, data_time: 0.004, memory: 6926, loss: 1.2129
2023-02-06 12:00:10,329 - mmcls - INFO - Epoch [3][240/782]	lr: 1.000e-03, eta: 0:26:31, time: 0.260, data_time: 0.004, memory: 6926, loss: 1.1554
2023-02-06 12:00:15,537 - mmcls - INFO - Epoch [3][260/782]	lr: 1.000e-03, eta: 0:26:26, time: 0.260, data_time: 0.004, memory: 6926, loss: 1.2004
2023-02-06 12:00:20,751 - mmcls - INFO - Epoch [3][280/782]	lr: 1.000e-03, eta: 0:26:20, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.1053
2023-02-06 12:00:25,965 - mmcls - INFO - Epoch [3][300/782]	lr: 1.000e-03, eta: 0:26:14, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.1747
2023-02-06 12:00:31,181 - mmcls - INFO - Epoch [3][320/782]	lr: 1.000e-03, eta: 0:26:09, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.2373
2023-02-06 12:00:36,400 - mmcls - INFO - Epoch [3][340/782]	lr: 1.000e-03, eta: 0:26:03, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.2717
2023-02-06 12:00:41,623 - mmcls - INFO - Epoch [3][360/782]	lr: 1.000e-03, eta: 0:25:58, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.2758
2023-02-06 12:00:46,844 - mmcls - INFO - Epoch [3][380/782]	lr: 1.000e-03, eta: 0:25:52, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.2126
2023-02-06 12:00:52,069 - mmcls - INFO - Epoch [3][400/782]	lr: 1.000e-03, eta: 0:25:47, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.3095
2023-02-06 12:00:57,293 - mmcls - INFO - Epoch [3][420/782]	lr: 1.000e-03, eta: 0:25:42, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.2876
2023-02-06 12:01:02,522 - mmcls - INFO - Epoch [3][440/782]	lr: 1.000e-03, eta: 0:25:36, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.0564
2023-02-06 12:01:07,750 - mmcls - INFO - Epoch [3][460/782]	lr: 1.000e-03, eta: 0:25:31, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.2518
2023-02-06 12:01:12,979 - mmcls - INFO - Epoch [3][480/782]	lr: 1.000e-03, eta: 0:25:25, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.2239
2023-02-06 12:01:18,209 - mmcls - INFO - Epoch [3][500/782]	lr: 1.000e-03, eta: 0:25:20, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.1078
2023-02-06 12:01:23,440 - mmcls - INFO - Epoch [3][520/782]	lr: 1.000e-03, eta: 0:25:14, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1637
2023-02-06 12:01:28,674 - mmcls - INFO - Epoch [3][540/782]	lr: 1.000e-03, eta: 0:25:09, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1670
2023-02-06 12:01:33,908 - mmcls - INFO - Epoch [3][560/782]	lr: 1.000e-03, eta: 0:25:04, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2284
2023-02-06 12:01:39,144 - mmcls - INFO - Epoch [3][580/782]	lr: 1.000e-03, eta: 0:24:58, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2869
2023-02-06 12:01:44,379 - mmcls - INFO - Epoch [3][600/782]	lr: 1.000e-03, eta: 0:24:53, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1921
2023-02-06 12:01:49,615 - mmcls - INFO - Epoch [3][620/782]	lr: 1.000e-03, eta: 0:24:47, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1163
2023-02-06 12:01:54,852 - mmcls - INFO - Epoch [3][640/782]	lr: 1.000e-03, eta: 0:24:42, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1274
2023-02-06 12:02:00,086 - mmcls - INFO - Epoch [3][660/782]	lr: 1.000e-03, eta: 0:24:37, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1684
2023-02-06 12:02:05,319 - mmcls - INFO - Epoch [3][680/782]	lr: 1.000e-03, eta: 0:24:31, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1715
2023-02-06 12:02:10,556 - mmcls - INFO - Epoch [3][700/782]	lr: 1.000e-03, eta: 0:24:26, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1226
2023-02-06 12:02:15,792 - mmcls - INFO - Epoch [3][720/782]	lr: 1.000e-03, eta: 0:24:20, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1728
2023-02-06 12:02:21,031 - mmcls - INFO - Epoch [3][740/782]	lr: 1.000e-03, eta: 0:24:15, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2092
2023-02-06 12:02:26,269 - mmcls - INFO - Epoch [3][760/782]	lr: 1.000e-03, eta: 0:24:10, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.0603
2023-02-06 12:02:31,507 - mmcls - INFO - Epoch [3][780/782]	lr: 1.000e-03, eta: 0:24:04, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1918
2023-02-06 12:02:31,819 - mmcls - INFO - Saving checkpoint at 3 epochs
2023-02-06 12:12:06,346 - mmcls - INFO - Epoch(val) [3][157]	accuracy_top-1: 96.0000, accuracy_top-5: 99.9700
2023-02-06 12:12:13,521 - mmcls - INFO - Epoch [4][20/782]	lr: 1.000e-04, eta: 0:24:01, time: 0.355, data_time: 0.103, memory: 6926, loss: 1.1319
2023-02-06 12:12:18,658 - mmcls - INFO - Epoch [4][40/782]	lr: 1.000e-04, eta: 0:23:56, time: 0.257, data_time: 0.004, memory: 6926, loss: 1.1683
2023-02-06 12:12:23,808 - mmcls - INFO - Epoch [4][60/782]	lr: 1.000e-04, eta: 0:23:50, time: 0.258, data_time: 0.004, memory: 6926, loss: 1.2615
2023-02-06 12:12:28,969 - mmcls - INFO - Epoch [4][80/782]	lr: 1.000e-04, eta: 0:23:45, time: 0.258, data_time: 0.004, memory: 6926, loss: 1.2512
2023-02-06 12:12:34,135 - mmcls - INFO - Epoch [4][100/782]	lr: 1.000e-04, eta: 0:23:39, time: 0.258, data_time: 0.004, memory: 6926, loss: 1.2295
2023-02-06 12:12:39,304 - mmcls - INFO - Epoch [4][120/782]	lr: 1.000e-04, eta: 0:23:34, time: 0.258, data_time: 0.004, memory: 6926, loss: 1.2486
2023-02-06 12:12:44,477 - mmcls - INFO - Epoch [4][140/782]	lr: 1.000e-04, eta: 0:23:28, time: 0.259, data_time: 0.004, memory: 6926, loss: 1.0785
2023-02-06 12:12:49,658 - mmcls - INFO - Epoch [4][160/782]	lr: 1.000e-04, eta: 0:23:23, time: 0.259, data_time: 0.004, memory: 6926, loss: 1.2119
2023-02-06 12:12:54,843 - mmcls - INFO - Epoch [4][180/782]	lr: 1.000e-04, eta: 0:23:17, time: 0.259, data_time: 0.004, memory: 6926, loss: 1.1666
2023-02-06 12:13:00,034 - mmcls - INFO - Epoch [4][200/782]	lr: 1.000e-04, eta: 0:23:12, time: 0.260, data_time: 0.004, memory: 6926, loss: 1.1471
2023-02-06 12:13:05,225 - mmcls - INFO - Epoch [4][220/782]	lr: 1.000e-04, eta: 0:23:06, time: 0.260, data_time: 0.004, memory: 6926, loss: 1.1879
2023-02-06 12:13:10,422 - mmcls - INFO - Epoch [4][240/782]	lr: 1.000e-04, eta: 0:23:01, time: 0.260, data_time: 0.004, memory: 6926, loss: 1.1713
2023-02-06 12:13:15,632 - mmcls - INFO - Epoch [4][260/782]	lr: 1.000e-04, eta: 0:22:55, time: 0.260, data_time: 0.004, memory: 6926, loss: 1.2290
2023-02-06 12:13:20,844 - mmcls - INFO - Epoch [4][280/782]	lr: 1.000e-04, eta: 0:22:50, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.1406
2023-02-06 12:13:26,058 - mmcls - INFO - Epoch [4][300/782]	lr: 1.000e-04, eta: 0:22:45, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.1373
2023-02-06 12:13:31,276 - mmcls - INFO - Epoch [4][320/782]	lr: 1.000e-04, eta: 0:22:39, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.1994
2023-02-06 12:13:36,499 - mmcls - INFO - Epoch [4][340/782]	lr: 1.000e-04, eta: 0:22:34, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.1766
2023-02-06 12:13:41,720 - mmcls - INFO - Epoch [4][360/782]	lr: 1.000e-04, eta: 0:22:28, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.1781
2023-02-06 12:13:46,946 - mmcls - INFO - Epoch [4][380/782]	lr: 1.000e-04, eta: 0:22:23, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.1925
2023-02-06 12:13:52,171 - mmcls - INFO - Epoch [4][400/782]	lr: 1.000e-04, eta: 0:22:18, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.1566
2023-02-06 12:13:57,400 - mmcls - INFO - Epoch [4][420/782]	lr: 1.000e-04, eta: 0:22:12, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.1763
2023-02-06 12:14:02,629 - mmcls - INFO - Epoch [4][440/782]	lr: 1.000e-04, eta: 0:22:07, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.2387
2023-02-06 12:14:07,860 - mmcls - INFO - Epoch [4][460/782]	lr: 1.000e-04, eta: 0:22:02, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1122
2023-02-06 12:14:13,091 - mmcls - INFO - Epoch [4][480/782]	lr: 1.000e-04, eta: 0:21:56, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1507
2023-02-06 12:14:18,322 - mmcls - INFO - Epoch [4][500/782]	lr: 1.000e-04, eta: 0:21:51, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1963
2023-02-06 12:14:23,553 - mmcls - INFO - Epoch [4][520/782]	lr: 1.000e-04, eta: 0:21:46, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1257
2023-02-06 12:14:28,785 - mmcls - INFO - Epoch [4][540/782]	lr: 1.000e-04, eta: 0:21:40, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1070
2023-02-06 12:14:34,019 - mmcls - INFO - Epoch [4][560/782]	lr: 1.000e-04, eta: 0:21:35, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1582
2023-02-06 12:14:39,254 - mmcls - INFO - Epoch [4][580/782]	lr: 1.000e-04, eta: 0:21:30, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1193
2023-02-06 12:14:44,489 - mmcls - INFO - Epoch [4][600/782]	lr: 1.000e-04, eta: 0:21:24, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2692
2023-02-06 12:14:49,725 - mmcls - INFO - Epoch [4][620/782]	lr: 1.000e-04, eta: 0:21:19, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2121
2023-02-06 12:14:54,958 - mmcls - INFO - Epoch [4][640/782]	lr: 1.000e-04, eta: 0:21:14, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2124
2023-02-06 12:15:00,195 - mmcls - INFO - Epoch [4][660/782]	lr: 1.000e-04, eta: 0:21:08, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2058
2023-02-06 12:15:05,433 - mmcls - INFO - Epoch [4][680/782]	lr: 1.000e-04, eta: 0:21:03, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1780
2023-02-06 12:15:10,667 - mmcls - INFO - Epoch [4][700/782]	lr: 1.000e-04, eta: 0:20:58, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2039
2023-02-06 12:15:15,903 - mmcls - INFO - Epoch [4][720/782]	lr: 1.000e-04, eta: 0:20:52, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1219
2023-02-06 12:15:21,143 - mmcls - INFO - Epoch [4][740/782]	lr: 1.000e-04, eta: 0:20:47, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.0803
2023-02-06 12:15:26,378 - mmcls - INFO - Epoch [4][760/782]	lr: 1.000e-04, eta: 0:20:42, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1527
2023-02-06 12:15:31,618 - mmcls - INFO - Epoch [4][780/782]	lr: 1.000e-04, eta: 0:20:36, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1902
2023-02-06 12:15:31,930 - mmcls - INFO - Saving checkpoint at 4 epochs
2023-02-06 12:20:15,106 - mmcls - INFO - Epoch(val) [4][157]	accuracy_top-1: 96.4800, accuracy_top-5: 99.9600
2023-02-06 12:20:22,302 - mmcls - INFO - Epoch [5][20/782]	lr: 1.000e-04, eta: 0:20:32, time: 0.356, data_time: 0.103, memory: 6926, loss: 1.0451
2023-02-06 12:20:27,452 - mmcls - INFO - Epoch [5][40/782]	lr: 1.000e-04, eta: 0:20:27, time: 0.257, data_time: 0.004, memory: 6926, loss: 1.1027
2023-02-06 12:20:32,613 - mmcls - INFO - Epoch [5][60/782]	lr: 1.000e-04, eta: 0:20:22, time: 0.258, data_time: 0.004, memory: 6926, loss: 1.1499
2023-02-06 12:20:37,780 - mmcls - INFO - Epoch [5][80/782]	lr: 1.000e-04, eta: 0:20:16, time: 0.258, data_time: 0.004, memory: 6926, loss: 1.1918
2023-02-06 12:20:42,946 - mmcls - INFO - Epoch [5][100/782]	lr: 1.000e-04, eta: 0:20:11, time: 0.258, data_time: 0.004, memory: 6926, loss: 1.2252
2023-02-06 12:20:48,121 - mmcls - INFO - Epoch [5][120/782]	lr: 1.000e-04, eta: 0:20:05, time: 0.259, data_time: 0.004, memory: 6926, loss: 1.2428
2023-02-06 12:20:53,300 - mmcls - INFO - Epoch [5][140/782]	lr: 1.000e-04, eta: 0:20:00, time: 0.259, data_time: 0.004, memory: 6926, loss: 1.1178
2023-02-06 12:20:58,483 - mmcls - INFO - Epoch [5][160/782]	lr: 1.000e-04, eta: 0:19:54, time: 0.259, data_time: 0.004, memory: 6926, loss: 1.0880
2023-02-06 12:21:03,672 - mmcls - INFO - Epoch [5][180/782]	lr: 1.000e-04, eta: 0:19:49, time: 0.259, data_time: 0.004, memory: 6926, loss: 1.2693
2023-02-06 12:21:08,868 - mmcls - INFO - Epoch [5][200/782]	lr: 1.000e-04, eta: 0:19:44, time: 0.260, data_time: 0.004, memory: 6926, loss: 1.2459
2023-02-06 12:21:14,076 - mmcls - INFO - Epoch [5][220/782]	lr: 1.000e-04, eta: 0:19:38, time: 0.260, data_time: 0.004, memory: 6926, loss: 1.1797
2023-02-06 12:21:19,287 - mmcls - INFO - Epoch [5][240/782]	lr: 1.000e-04, eta: 0:19:33, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.0615
2023-02-06 12:21:24,502 - mmcls - INFO - Epoch [5][260/782]	lr: 1.000e-04, eta: 0:19:28, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.1846
2023-02-06 12:21:29,717 - mmcls - INFO - Epoch [5][280/782]	lr: 1.000e-04, eta: 0:19:22, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.2342
2023-02-06 12:21:34,934 - mmcls - INFO - Epoch [5][300/782]	lr: 1.000e-04, eta: 0:19:17, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.0612
2023-02-06 12:21:40,156 - mmcls - INFO - Epoch [5][320/782]	lr: 1.000e-04, eta: 0:19:12, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.1413
2023-02-06 12:21:45,380 - mmcls - INFO - Epoch [5][340/782]	lr: 1.000e-04, eta: 0:19:06, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.1165
2023-02-06 12:21:50,608 - mmcls - INFO - Epoch [5][360/782]	lr: 1.000e-04, eta: 0:19:01, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.1616
2023-02-06 12:21:55,836 - mmcls - INFO - Epoch [5][380/782]	lr: 1.000e-04, eta: 0:18:56, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.1774
2023-02-06 12:22:01,065 - mmcls - INFO - Epoch [5][400/782]	lr: 1.000e-04, eta: 0:18:50, time: 0.261, data_time: 0.004, memory: 6926, loss: 1.1437
2023-02-06 12:22:06,297 - mmcls - INFO - Epoch [5][420/782]	lr: 1.000e-04, eta: 0:18:45, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.0560
2023-02-06 12:22:11,530 - mmcls - INFO - Epoch [5][440/782]	lr: 1.000e-04, eta: 0:18:40, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1486
2023-02-06 12:22:16,760 - mmcls - INFO - Epoch [5][460/782]	lr: 1.000e-04, eta: 0:18:34, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1443
2023-02-06 12:22:21,995 - mmcls - INFO - Epoch [5][480/782]	lr: 1.000e-04, eta: 0:18:29, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2492
2023-02-06 12:22:27,231 - mmcls - INFO - Epoch [5][500/782]	lr: 1.000e-04, eta: 0:18:24, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1177
2023-02-06 12:22:32,464 - mmcls - INFO - Epoch [5][520/782]	lr: 1.000e-04, eta: 0:18:18, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.0813
2023-02-06 12:22:37,699 - mmcls - INFO - Epoch [5][540/782]	lr: 1.000e-04, eta: 0:18:13, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2538
2023-02-06 12:22:42,933 - mmcls - INFO - Epoch [5][560/782]	lr: 1.000e-04, eta: 0:18:08, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1734
2023-02-06 12:22:48,168 - mmcls - INFO - Epoch [5][580/782]	lr: 1.000e-04, eta: 0:18:03, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1776
2023-02-06 12:22:53,405 - mmcls - INFO - Epoch [5][600/782]	lr: 1.000e-04, eta: 0:17:57, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1146
2023-02-06 12:22:58,642 - mmcls - INFO - Epoch [5][620/782]	lr: 1.000e-04, eta: 0:17:52, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.0904
2023-02-06 12:23:03,878 - mmcls - INFO - Epoch [5][640/782]	lr: 1.000e-04, eta: 0:17:47, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2026
2023-02-06 12:23:09,115 - mmcls - INFO - Epoch [5][660/782]	lr: 1.000e-04, eta: 0:17:41, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2440
2023-02-06 12:23:14,352 - mmcls - INFO - Epoch [5][680/782]	lr: 1.000e-04, eta: 0:17:36, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1064
2023-02-06 12:23:19,589 - mmcls - INFO - Epoch [5][700/782]	lr: 1.000e-04, eta: 0:17:31, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1674
2023-02-06 12:23:24,831 - mmcls - INFO - Epoch [5][720/782]	lr: 1.000e-04, eta: 0:17:25, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1192
2023-02-06 12:23:30,070 - mmcls - INFO - Epoch [5][740/782]	lr: 1.000e-04, eta: 0:17:20, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1412
2023-02-06 12:23:35,308 - mmcls - INFO - Epoch [5][760/782]	lr: 1.000e-04, eta: 0:17:15, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1927
2023-02-06 12:23:40,548 - mmcls - INFO - Epoch [5][780/782]	lr: 1.000e-04, eta: 0:17:10, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2295
2023-02-06 12:23:40,860 - mmcls - INFO - Saving checkpoint at 5 epochs
2023-02-06 12:23:54,715 - mmcls - INFO - Epoch(val) [5][157]	accuracy_top-1: 96.6000, accuracy_top-5: 99.9600
2023-02-06 12:24:01,996 - mmcls - INFO - Epoch [6][20/782]	lr: 1.000e-04, eta: 0:17:05, time: 0.360, data_time: 0.103, memory: 6926, loss: 1.0387
2023-02-06 12:24:07,233 - mmcls - INFO - Epoch [6][40/782]	lr: 1.000e-04, eta: 0:17:00, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1539
2023-02-06 12:24:12,473 - mmcls - INFO - Epoch [6][60/782]	lr: 1.000e-04, eta: 0:16:55, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.0793
2023-02-06 12:24:17,710 - mmcls - INFO - Epoch [6][80/782]	lr: 1.000e-04, eta: 0:16:49, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1737
2023-02-06 12:24:22,952 - mmcls - INFO - Epoch [6][100/782]	lr: 1.000e-04, eta: 0:16:44, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1295
2023-02-06 12:24:28,192 - mmcls - INFO - Epoch [6][120/782]	lr: 1.000e-04, eta: 0:16:39, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1711
2023-02-06 12:24:33,433 - mmcls - INFO - Epoch [6][140/782]	lr: 1.000e-04, eta: 0:16:33, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1519
2023-02-06 12:24:38,675 - mmcls - INFO - Epoch [6][160/782]	lr: 1.000e-04, eta: 0:16:28, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1565
2023-02-06 12:24:43,917 - mmcls - INFO - Epoch [6][180/782]	lr: 1.000e-04, eta: 0:16:23, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2163
2023-02-06 12:24:49,160 - mmcls - INFO - Epoch [6][200/782]	lr: 1.000e-04, eta: 0:16:17, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1619
2023-02-06 12:24:54,403 - mmcls - INFO - Epoch [6][220/782]	lr: 1.000e-04, eta: 0:16:12, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1654
2023-02-06 12:24:59,644 - mmcls - INFO - Epoch [6][240/782]	lr: 1.000e-04, eta: 0:16:07, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1283
2023-02-06 12:25:04,885 - mmcls - INFO - Epoch [6][260/782]	lr: 1.000e-04, eta: 0:16:02, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.0764
2023-02-06 12:25:10,130 - mmcls - INFO - Epoch [6][280/782]	lr: 1.000e-04, eta: 0:15:56, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1398
2023-02-06 12:25:15,372 - mmcls - INFO - Epoch [6][300/782]	lr: 1.000e-04, eta: 0:15:51, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1750
2023-02-06 12:25:20,615 - mmcls - INFO - Epoch [6][320/782]	lr: 1.000e-04, eta: 0:15:46, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2146
2023-02-06 12:25:25,857 - mmcls - INFO - Epoch [6][340/782]	lr: 1.000e-04, eta: 0:15:40, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1393
2023-02-06 12:25:31,100 - mmcls - INFO - Epoch [6][360/782]	lr: 1.000e-04, eta: 0:15:35, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1834
2023-02-06 12:25:36,341 - mmcls - INFO - Epoch [6][380/782]	lr: 1.000e-04, eta: 0:15:30, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.0865
2023-02-06 12:25:41,583 - mmcls - INFO - Epoch [6][400/782]	lr: 1.000e-04, eta: 0:15:24, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1965
2023-02-06 12:25:46,829 - mmcls - INFO - Epoch [6][420/782]	lr: 1.000e-04, eta: 0:15:19, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2353
2023-02-06 12:25:52,073 - mmcls - INFO - Epoch [6][440/782]	lr: 1.000e-04, eta: 0:15:14, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2484
2023-02-06 12:25:57,320 - mmcls - INFO - Epoch [6][460/782]	lr: 1.000e-04, eta: 0:15:09, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2093
2023-02-06 12:26:02,565 - mmcls - INFO - Epoch [6][480/782]	lr: 1.000e-04, eta: 0:15:03, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1592
2023-02-06 12:26:07,807 - mmcls - INFO - Epoch [6][500/782]	lr: 1.000e-04, eta: 0:14:58, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.0553
2023-02-06 12:26:13,053 - mmcls - INFO - Epoch [6][520/782]	lr: 1.000e-04, eta: 0:14:53, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1585
2023-02-06 12:26:18,298 - mmcls - INFO - Epoch [6][540/782]	lr: 1.000e-04, eta: 0:14:47, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1422
2023-02-06 12:26:23,544 - mmcls - INFO - Epoch [6][560/782]	lr: 1.000e-04, eta: 0:14:42, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1490
2023-02-06 12:26:28,788 - mmcls - INFO - Epoch [6][580/782]	lr: 1.000e-04, eta: 0:14:37, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1035
2023-02-06 12:26:34,034 - mmcls - INFO - Epoch [6][600/782]	lr: 1.000e-04, eta: 0:14:32, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1121
2023-02-06 12:26:39,278 - mmcls - INFO - Epoch [6][620/782]	lr: 1.000e-04, eta: 0:14:26, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1264
2023-02-06 12:26:44,520 - mmcls - INFO - Epoch [6][640/782]	lr: 1.000e-04, eta: 0:14:21, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1623
2023-02-06 12:26:49,763 - mmcls - INFO - Epoch [6][660/782]	lr: 1.000e-04, eta: 0:14:16, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1731
2023-02-06 12:26:55,007 - mmcls - INFO - Epoch [6][680/782]	lr: 1.000e-04, eta: 0:14:10, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2120
2023-02-06 12:27:00,250 - mmcls - INFO - Epoch [6][700/782]	lr: 1.000e-04, eta: 0:14:05, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1803
2023-02-06 12:27:05,492 - mmcls - INFO - Epoch [6][720/782]	lr: 1.000e-04, eta: 0:14:00, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1232
2023-02-06 12:27:10,734 - mmcls - INFO - Epoch [6][740/782]	lr: 1.000e-04, eta: 0:13:55, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1604
2023-02-06 12:27:15,977 - mmcls - INFO - Epoch [6][760/782]	lr: 1.000e-04, eta: 0:13:49, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1471
2023-02-06 12:27:21,219 - mmcls - INFO - Epoch [6][780/782]	lr: 1.000e-04, eta: 0:13:44, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1399
2023-02-06 12:27:21,532 - mmcls - INFO - Saving checkpoint at 6 epochs
2023-02-06 12:27:35,388 - mmcls - INFO - Epoch(val) [6][157]	accuracy_top-1: 96.7200, accuracy_top-5: 99.9500
2023-02-06 12:27:42,676 - mmcls - INFO - Epoch [7][20/782]	lr: 1.000e-05, eta: 0:13:39, time: 0.360, data_time: 0.103, memory: 6926, loss: 1.1250
2023-02-06 12:27:47,915 - mmcls - INFO - Epoch [7][40/782]	lr: 1.000e-05, eta: 0:13:34, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2146
2023-02-06 12:27:53,154 - mmcls - INFO - Epoch [7][60/782]	lr: 1.000e-05, eta: 0:13:29, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2038
2023-02-06 12:27:58,392 - mmcls - INFO - Epoch [7][80/782]	lr: 1.000e-05, eta: 0:13:23, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.0750
2023-02-06 12:28:03,633 - mmcls - INFO - Epoch [7][100/782]	lr: 1.000e-05, eta: 0:13:18, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.0922
2023-02-06 12:28:08,874 - mmcls - INFO - Epoch [7][120/782]	lr: 1.000e-05, eta: 0:13:13, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1128
2023-02-06 12:28:14,118 - mmcls - INFO - Epoch [7][140/782]	lr: 1.000e-05, eta: 0:13:07, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2259
2023-02-06 12:28:19,362 - mmcls - INFO - Epoch [7][160/782]	lr: 1.000e-05, eta: 0:13:02, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1351
2023-02-06 12:28:24,604 - mmcls - INFO - Epoch [7][180/782]	lr: 1.000e-05, eta: 0:12:57, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1138
2023-02-06 12:28:29,850 - mmcls - INFO - Epoch [7][200/782]	lr: 1.000e-05, eta: 0:12:51, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.0765
2023-02-06 12:28:35,090 - mmcls - INFO - Epoch [7][220/782]	lr: 1.000e-05, eta: 0:12:46, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1233
2023-02-06 12:28:40,329 - mmcls - INFO - Epoch [7][240/782]	lr: 1.000e-05, eta: 0:12:41, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1714
2023-02-06 12:28:45,572 - mmcls - INFO - Epoch [7][260/782]	lr: 1.000e-05, eta: 0:12:36, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2057
2023-02-06 12:28:50,812 - mmcls - INFO - Epoch [7][280/782]	lr: 1.000e-05, eta: 0:12:30, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1064
2023-02-06 12:28:56,054 - mmcls - INFO - Epoch [7][300/782]	lr: 1.000e-05, eta: 0:12:25, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2069
2023-02-06 12:29:01,297 - mmcls - INFO - Epoch [7][320/782]	lr: 1.000e-05, eta: 0:12:20, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.0862
2023-02-06 12:29:06,539 - mmcls - INFO - Epoch [7][340/782]	lr: 1.000e-05, eta: 0:12:14, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2068
2023-02-06 12:29:11,782 - mmcls - INFO - Epoch [7][360/782]	lr: 1.000e-05, eta: 0:12:09, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1445
2023-02-06 12:29:17,023 - mmcls - INFO - Epoch [7][380/782]	lr: 1.000e-05, eta: 0:12:04, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2384
2023-02-06 12:29:22,268 - mmcls - INFO - Epoch [7][400/782]	lr: 1.000e-05, eta: 0:11:59, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1415
2023-02-06 12:29:27,511 - mmcls - INFO - Epoch [7][420/782]	lr: 1.000e-05, eta: 0:11:53, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1884
2023-02-06 12:29:32,753 - mmcls - INFO - Epoch [7][440/782]	lr: 1.000e-05, eta: 0:11:48, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1807
2023-02-06 12:29:37,995 - mmcls - INFO - Epoch [7][460/782]	lr: 1.000e-05, eta: 0:11:43, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1384
2023-02-06 12:29:43,239 - mmcls - INFO - Epoch [7][480/782]	lr: 1.000e-05, eta: 0:11:37, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2412
2023-02-06 12:29:48,481 - mmcls - INFO - Epoch [7][500/782]	lr: 1.000e-05, eta: 0:11:32, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1606
2023-02-06 12:29:53,725 - mmcls - INFO - Epoch [7][520/782]	lr: 1.000e-05, eta: 0:11:27, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2430
2023-02-06 12:29:58,967 - mmcls - INFO - Epoch [7][540/782]	lr: 1.000e-05, eta: 0:11:22, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.0796
2023-02-06 12:30:04,208 - mmcls - INFO - Epoch [7][560/782]	lr: 1.000e-05, eta: 0:11:16, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1476
2023-02-06 12:30:09,450 - mmcls - INFO - Epoch [7][580/782]	lr: 1.000e-05, eta: 0:11:11, time: 0.262, data_time: 0.004, memory: 6926, loss: 0.9958
2023-02-06 12:30:14,692 - mmcls - INFO - Epoch [7][600/782]	lr: 1.000e-05, eta: 0:11:06, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1976
2023-02-06 12:30:19,936 - mmcls - INFO - Epoch [7][620/782]	lr: 1.000e-05, eta: 0:11:00, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1697
2023-02-06 12:30:25,180 - mmcls - INFO - Epoch [7][640/782]	lr: 1.000e-05, eta: 0:10:55, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2314
2023-02-06 12:30:30,426 - mmcls - INFO - Epoch [7][660/782]	lr: 1.000e-05, eta: 0:10:50, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1644
2023-02-06 12:30:35,670 - mmcls - INFO - Epoch [7][680/782]	lr: 1.000e-05, eta: 0:10:45, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1340
2023-02-06 12:30:40,913 - mmcls - INFO - Epoch [7][700/782]	lr: 1.000e-05, eta: 0:10:39, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1088
2023-02-06 12:30:46,160 - mmcls - INFO - Epoch [7][720/782]	lr: 1.000e-05, eta: 0:10:34, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1729
2023-02-06 12:30:51,405 - mmcls - INFO - Epoch [7][740/782]	lr: 1.000e-05, eta: 0:10:29, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1601
2023-02-06 12:30:56,647 - mmcls - INFO - Epoch [7][760/782]	lr: 1.000e-05, eta: 0:10:23, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1690
2023-02-06 12:31:01,890 - mmcls - INFO - Epoch [7][780/782]	lr: 1.000e-05, eta: 0:10:18, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1097
2023-02-06 12:31:02,202 - mmcls - INFO - Saving checkpoint at 7 epochs
2023-02-06 12:31:16,067 - mmcls - INFO - Epoch(val) [7][157]	accuracy_top-1: 96.8100, accuracy_top-5: 99.9500
2023-02-06 12:31:23,352 - mmcls - INFO - Epoch [8][20/782]	lr: 1.000e-05, eta: 0:10:13, time: 0.360, data_time: 0.103, memory: 6926, loss: 1.1404
2023-02-06 12:31:28,588 - mmcls - INFO - Epoch [8][40/782]	lr: 1.000e-05, eta: 0:10:08, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1195
2023-02-06 12:31:33,827 - mmcls - INFO - Epoch [8][60/782]	lr: 1.000e-05, eta: 0:10:02, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2768
2023-02-06 12:31:39,069 - mmcls - INFO - Epoch [8][80/782]	lr: 1.000e-05, eta: 0:09:57, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1237
2023-02-06 12:31:44,309 - mmcls - INFO - Epoch [8][100/782]	lr: 1.000e-05, eta: 0:09:52, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.0132
2023-02-06 12:31:49,547 - mmcls - INFO - Epoch [8][120/782]	lr: 1.000e-05, eta: 0:09:47, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1021
2023-02-06 12:31:54,786 - mmcls - INFO - Epoch [8][140/782]	lr: 1.000e-05, eta: 0:09:41, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1951
2023-02-06 12:32:00,027 - mmcls - INFO - Epoch [8][160/782]	lr: 1.000e-05, eta: 0:09:36, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1811
2023-02-06 12:32:05,265 - mmcls - INFO - Epoch [8][180/782]	lr: 1.000e-05, eta: 0:09:31, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1859
2023-02-06 12:32:10,507 - mmcls - INFO - Epoch [8][200/782]	lr: 1.000e-05, eta: 0:09:25, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1220
2023-02-06 12:32:15,749 - mmcls - INFO - Epoch [8][220/782]	lr: 1.000e-05, eta: 0:09:20, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1172
2023-02-06 12:32:20,990 - mmcls - INFO - Epoch [8][240/782]	lr: 1.000e-05, eta: 0:09:15, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1112
2023-02-06 12:32:26,233 - mmcls - INFO - Epoch [8][260/782]	lr: 1.000e-05, eta: 0:09:10, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1286
2023-02-06 12:32:31,475 - mmcls - INFO - Epoch [8][280/782]	lr: 1.000e-05, eta: 0:09:04, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2051
2023-02-06 12:32:36,716 - mmcls - INFO - Epoch [8][300/782]	lr: 1.000e-05, eta: 0:08:59, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1656
2023-02-06 12:32:41,957 - mmcls - INFO - Epoch [8][320/782]	lr: 1.000e-05, eta: 0:08:54, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1924
2023-02-06 12:32:47,200 - mmcls - INFO - Epoch [8][340/782]	lr: 1.000e-05, eta: 0:08:48, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1970
2023-02-06 12:32:52,445 - mmcls - INFO - Epoch [8][360/782]	lr: 1.000e-05, eta: 0:08:43, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1098
2023-02-06 12:32:57,689 - mmcls - INFO - Epoch [8][380/782]	lr: 1.000e-05, eta: 0:08:38, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.0326
2023-02-06 12:33:02,935 - mmcls - INFO - Epoch [8][400/782]	lr: 1.000e-05, eta: 0:08:33, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2301
2023-02-06 12:33:08,176 - mmcls - INFO - Epoch [8][420/782]	lr: 1.000e-05, eta: 0:08:27, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.0865
2023-02-06 12:33:13,421 - mmcls - INFO - Epoch [8][440/782]	lr: 1.000e-05, eta: 0:08:22, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1447
2023-02-06 12:33:18,665 - mmcls - INFO - Epoch [8][460/782]	lr: 1.000e-05, eta: 0:08:17, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1983
2023-02-06 12:33:23,908 - mmcls - INFO - Epoch [8][480/782]	lr: 1.000e-05, eta: 0:08:11, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1569
2023-02-06 12:33:29,152 - mmcls - INFO - Epoch [8][500/782]	lr: 1.000e-05, eta: 0:08:06, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.0642
2023-02-06 12:33:34,396 - mmcls - INFO - Epoch [8][520/782]	lr: 1.000e-05, eta: 0:08:01, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2061
2023-02-06 12:33:39,639 - mmcls - INFO - Epoch [8][540/782]	lr: 1.000e-05, eta: 0:07:56, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.3010
2023-02-06 12:33:44,881 - mmcls - INFO - Epoch [8][560/782]	lr: 1.000e-05, eta: 0:07:50, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1501
2023-02-06 12:33:50,125 - mmcls - INFO - Epoch [8][580/782]	lr: 1.000e-05, eta: 0:07:45, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1238
2023-02-06 12:33:55,368 - mmcls - INFO - Epoch [8][600/782]	lr: 1.000e-05, eta: 0:07:40, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.0548
2023-02-06 12:34:00,613 - mmcls - INFO - Epoch [8][620/782]	lr: 1.000e-05, eta: 0:07:34, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.0109
2023-02-06 12:34:05,857 - mmcls - INFO - Epoch [8][640/782]	lr: 1.000e-05, eta: 0:07:29, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1780
2023-02-06 12:34:11,099 - mmcls - INFO - Epoch [8][660/782]	lr: 1.000e-05, eta: 0:07:24, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1111
2023-02-06 12:34:16,344 - mmcls - INFO - Epoch [8][680/782]	lr: 1.000e-05, eta: 0:07:19, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1087
2023-02-06 12:34:21,588 - mmcls - INFO - Epoch [8][700/782]	lr: 1.000e-05, eta: 0:07:13, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1188
2023-02-06 12:34:26,829 - mmcls - INFO - Epoch [8][720/782]	lr: 1.000e-05, eta: 0:07:08, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1052
2023-02-06 12:34:32,073 - mmcls - INFO - Epoch [8][740/782]	lr: 1.000e-05, eta: 0:07:03, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.0272
2023-02-06 12:34:37,314 - mmcls - INFO - Epoch [8][760/782]	lr: 1.000e-05, eta: 0:06:57, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1553
2023-02-06 12:34:42,558 - mmcls - INFO - Epoch [8][780/782]	lr: 1.000e-05, eta: 0:06:52, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1523
2023-02-06 12:34:42,871 - mmcls - INFO - Saving checkpoint at 8 epochs
2023-02-06 12:34:56,749 - mmcls - INFO - Epoch(val) [8][157]	accuracy_top-1: 96.7600, accuracy_top-5: 99.9500
2023-02-06 12:35:04,034 - mmcls - INFO - Epoch [9][20/782]	lr: 1.000e-05, eta: 0:06:47, time: 0.360, data_time: 0.103, memory: 6926, loss: 1.1476
2023-02-06 12:35:09,272 - mmcls - INFO - Epoch [9][40/782]	lr: 1.000e-05, eta: 0:06:41, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1506
2023-02-06 12:35:14,512 - mmcls - INFO - Epoch [9][60/782]	lr: 1.000e-05, eta: 0:06:36, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1735
2023-02-06 12:35:19,750 - mmcls - INFO - Epoch [9][80/782]	lr: 1.000e-05, eta: 0:06:31, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1303
2023-02-06 12:35:24,991 - mmcls - INFO - Epoch [9][100/782]	lr: 1.000e-05, eta: 0:06:26, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1285
2023-02-06 12:35:30,232 - mmcls - INFO - Epoch [9][120/782]	lr: 1.000e-05, eta: 0:06:20, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1751
2023-02-06 12:35:35,473 - mmcls - INFO - Epoch [9][140/782]	lr: 1.000e-05, eta: 0:06:15, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1029
2023-02-06 12:35:40,712 - mmcls - INFO - Epoch [9][160/782]	lr: 1.000e-05, eta: 0:06:10, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.0836
2023-02-06 12:35:45,954 - mmcls - INFO - Epoch [9][180/782]	lr: 1.000e-05, eta: 0:06:05, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2283
2023-02-06 12:35:51,196 - mmcls - INFO - Epoch [9][200/782]	lr: 1.000e-05, eta: 0:05:59, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1380
2023-02-06 12:35:56,438 - mmcls - INFO - Epoch [9][220/782]	lr: 1.000e-05, eta: 0:05:54, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1543
2023-02-06 12:36:01,678 - mmcls - INFO - Epoch [9][240/782]	lr: 1.000e-05, eta: 0:05:49, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.0707
2023-02-06 12:36:06,917 - mmcls - INFO - Epoch [9][260/782]	lr: 1.000e-05, eta: 0:05:43, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1167
2023-02-06 12:36:12,160 - mmcls - INFO - Epoch [9][280/782]	lr: 1.000e-05, eta: 0:05:38, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1767
2023-02-06 12:36:17,401 - mmcls - INFO - Epoch [9][300/782]	lr: 1.000e-05, eta: 0:05:33, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1212
2023-02-06 12:36:22,644 - mmcls - INFO - Epoch [9][320/782]	lr: 1.000e-05, eta: 0:05:28, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.0974
2023-02-06 12:36:27,885 - mmcls - INFO - Epoch [9][340/782]	lr: 1.000e-05, eta: 0:05:22, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.0787
2023-02-06 12:36:33,128 - mmcls - INFO - Epoch [9][360/782]	lr: 1.000e-05, eta: 0:05:17, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1110
2023-02-06 12:36:38,371 - mmcls - INFO - Epoch [9][380/782]	lr: 1.000e-05, eta: 0:05:12, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2197
2023-02-06 12:36:43,613 - mmcls - INFO - Epoch [9][400/782]	lr: 1.000e-05, eta: 0:05:06, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.0766
2023-02-06 12:36:48,854 - mmcls - INFO - Epoch [9][420/782]	lr: 1.000e-05, eta: 0:05:01, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1737
2023-02-06 12:36:54,097 - mmcls - INFO - Epoch [9][440/782]	lr: 1.000e-05, eta: 0:04:56, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2710
2023-02-06 12:36:59,343 - mmcls - INFO - Epoch [9][460/782]	lr: 1.000e-05, eta: 0:04:51, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1563
2023-02-06 12:37:04,583 - mmcls - INFO - Epoch [9][480/782]	lr: 1.000e-05, eta: 0:04:45, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1562
2023-02-06 12:37:09,827 - mmcls - INFO - Epoch [9][500/782]	lr: 1.000e-05, eta: 0:04:40, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1205
2023-02-06 12:37:15,072 - mmcls - INFO - Epoch [9][520/782]	lr: 1.000e-05, eta: 0:04:35, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1315
2023-02-06 12:37:20,315 - mmcls - INFO - Epoch [9][540/782]	lr: 1.000e-05, eta: 0:04:29, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1490
2023-02-06 12:37:25,557 - mmcls - INFO - Epoch [9][560/782]	lr: 1.000e-05, eta: 0:04:24, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1655
2023-02-06 12:37:30,800 - mmcls - INFO - Epoch [9][580/782]	lr: 1.000e-05, eta: 0:04:19, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.0380
2023-02-06 12:37:36,043 - mmcls - INFO - Epoch [9][600/782]	lr: 1.000e-05, eta: 0:04:14, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2381
2023-02-06 12:37:41,287 - mmcls - INFO - Epoch [9][620/782]	lr: 1.000e-05, eta: 0:04:08, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1992
2023-02-06 12:37:46,529 - mmcls - INFO - Epoch [9][640/782]	lr: 1.000e-05, eta: 0:04:03, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1854
2023-02-06 12:37:51,772 - mmcls - INFO - Epoch [9][660/782]	lr: 1.000e-05, eta: 0:03:58, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.0047
2023-02-06 12:37:57,018 - mmcls - INFO - Epoch [9][680/782]	lr: 1.000e-05, eta: 0:03:53, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2250
2023-02-06 12:38:02,260 - mmcls - INFO - Epoch [9][700/782]	lr: 1.000e-05, eta: 0:03:47, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1530
2023-02-06 12:38:07,504 - mmcls - INFO - Epoch [9][720/782]	lr: 1.000e-05, eta: 0:03:42, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1119
2023-02-06 12:38:12,746 - mmcls - INFO - Epoch [9][740/782]	lr: 1.000e-05, eta: 0:03:37, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2604
2023-02-06 12:38:17,988 - mmcls - INFO - Epoch [9][760/782]	lr: 1.000e-05, eta: 0:03:31, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2424
2023-02-06 12:38:23,231 - mmcls - INFO - Epoch [9][780/782]	lr: 1.000e-05, eta: 0:03:26, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1296
2023-02-06 12:38:23,544 - mmcls - INFO - Saving checkpoint at 9 epochs
2023-02-06 12:38:37,399 - mmcls - INFO - Epoch(val) [9][157]	accuracy_top-1: 96.8200, accuracy_top-5: 99.9500
2023-02-06 12:38:44,684 - mmcls - INFO - Epoch [10][20/782]	lr: 1.000e-06, eta: 0:03:21, time: 0.360, data_time: 0.103, memory: 6926, loss: 1.1593
2023-02-06 12:38:49,924 - mmcls - INFO - Epoch [10][40/782]	lr: 1.000e-06, eta: 0:03:15, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2532
2023-02-06 12:38:55,164 - mmcls - INFO - Epoch [10][60/782]	lr: 1.000e-06, eta: 0:03:10, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1426
2023-02-06 12:39:00,402 - mmcls - INFO - Epoch [10][80/782]	lr: 1.000e-06, eta: 0:03:05, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2113
2023-02-06 12:39:05,643 - mmcls - INFO - Epoch [10][100/782]	lr: 1.000e-06, eta: 0:02:59, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1684
2023-02-06 12:39:10,884 - mmcls - INFO - Epoch [10][120/782]	lr: 1.000e-06, eta: 0:02:54, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1493
2023-02-06 12:39:16,125 - mmcls - INFO - Epoch [10][140/782]	lr: 1.000e-06, eta: 0:02:49, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1290
2023-02-06 12:39:21,370 - mmcls - INFO - Epoch [10][160/782]	lr: 1.000e-06, eta: 0:02:44, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1583
2023-02-06 12:39:26,612 - mmcls - INFO - Epoch [10][180/782]	lr: 1.000e-06, eta: 0:02:38, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1944
2023-02-06 12:39:31,853 - mmcls - INFO - Epoch [10][200/782]	lr: 1.000e-06, eta: 0:02:33, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1695
2023-02-06 12:39:37,097 - mmcls - INFO - Epoch [10][220/782]	lr: 1.000e-06, eta: 0:02:28, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1674
2023-02-06 12:39:42,338 - mmcls - INFO - Epoch [10][240/782]	lr: 1.000e-06, eta: 0:02:22, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1536
2023-02-06 12:39:47,583 - mmcls - INFO - Epoch [10][260/782]	lr: 1.000e-06, eta: 0:02:17, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1542
2023-02-06 12:39:52,827 - mmcls - INFO - Epoch [10][280/782]	lr: 1.000e-06, eta: 0:02:12, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1427
2023-02-06 12:39:58,071 - mmcls - INFO - Epoch [10][300/782]	lr: 1.000e-06, eta: 0:02:07, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1961
2023-02-06 12:40:03,317 - mmcls - INFO - Epoch [10][320/782]	lr: 1.000e-06, eta: 0:02:01, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2067
2023-02-06 12:40:08,560 - mmcls - INFO - Epoch [10][340/782]	lr: 1.000e-06, eta: 0:01:56, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1573
2023-02-06 12:40:13,805 - mmcls - INFO - Epoch [10][360/782]	lr: 1.000e-06, eta: 0:01:51, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1267
2023-02-06 12:40:19,051 - mmcls - INFO - Epoch [10][380/782]	lr: 1.000e-06, eta: 0:01:46, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2264
2023-02-06 12:40:24,293 - mmcls - INFO - Epoch [10][400/782]	lr: 1.000e-06, eta: 0:01:40, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2106
2023-02-06 12:40:29,539 - mmcls - INFO - Epoch [10][420/782]	lr: 1.000e-06, eta: 0:01:35, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1403
2023-02-06 12:40:34,783 - mmcls - INFO - Epoch [10][440/782]	lr: 1.000e-06, eta: 0:01:30, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1337
2023-02-06 12:40:40,028 - mmcls - INFO - Epoch [10][460/782]	lr: 1.000e-06, eta: 0:01:24, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.0709
2023-02-06 12:40:45,271 - mmcls - INFO - Epoch [10][480/782]	lr: 1.000e-06, eta: 0:01:19, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2273
2023-02-06 12:40:50,514 - mmcls - INFO - Epoch [10][500/782]	lr: 1.000e-06, eta: 0:01:14, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1693
2023-02-06 12:40:55,758 - mmcls - INFO - Epoch [10][520/782]	lr: 1.000e-06, eta: 0:01:09, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2016
2023-02-06 12:41:01,004 - mmcls - INFO - Epoch [10][540/782]	lr: 1.000e-06, eta: 0:01:03, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1103
2023-02-06 12:41:06,248 - mmcls - INFO - Epoch [10][560/782]	lr: 1.000e-06, eta: 0:00:58, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1747
2023-02-06 12:41:11,490 - mmcls - INFO - Epoch [10][580/782]	lr: 1.000e-06, eta: 0:00:53, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1953
2023-02-06 12:41:16,732 - mmcls - INFO - Epoch [10][600/782]	lr: 1.000e-06, eta: 0:00:47, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.0101
2023-02-06 12:41:21,977 - mmcls - INFO - Epoch [10][620/782]	lr: 1.000e-06, eta: 0:00:42, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1361
2023-02-06 12:41:27,220 - mmcls - INFO - Epoch [10][640/782]	lr: 1.000e-06, eta: 0:00:37, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1488
2023-02-06 12:41:32,466 - mmcls - INFO - Epoch [10][660/782]	lr: 1.000e-06, eta: 0:00:32, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.0912
2023-02-06 12:41:37,709 - mmcls - INFO - Epoch [10][680/782]	lr: 1.000e-06, eta: 0:00:26, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1047
2023-02-06 12:41:42,953 - mmcls - INFO - Epoch [10][700/782]	lr: 1.000e-06, eta: 0:00:21, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1804
2023-02-06 12:41:48,195 - mmcls - INFO - Epoch [10][720/782]	lr: 1.000e-06, eta: 0:00:16, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.2192
2023-02-06 12:41:53,439 - mmcls - INFO - Epoch [10][740/782]	lr: 1.000e-06, eta: 0:00:11, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1615
2023-02-06 12:41:58,681 - mmcls - INFO - Epoch [10][760/782]	lr: 1.000e-06, eta: 0:00:05, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1765
2023-02-06 12:42:03,926 - mmcls - INFO - Epoch [10][780/782]	lr: 1.000e-06, eta: 0:00:00, time: 0.262, data_time: 0.004, memory: 6926, loss: 1.1680
2023-02-06 12:42:04,239 - mmcls - INFO - Saving checkpoint at 10 epochs
2023-02-06 12:42:18,117 - mmcls - INFO - Epoch(val) [10][157]	accuracy_top-1: 96.8300, accuracy_top-5: 99.9500
